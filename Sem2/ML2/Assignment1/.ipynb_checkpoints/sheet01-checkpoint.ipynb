{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "In this exercise, you will implement elements of the t-SNE algorithm described in the paper by Laurens van der Maaten (available on ISIS), and analyze its behavior. As a reminder, here are the main steps of the t-SNE procedure described in the paper:\n",
    "\n",
    "* compute pairwise affinities $p_{j|i}$ with perplexity perp using $p_{j|i} = \\frac{\\exp(-||x_i - x_j||^2/2\\sigma^2)}{\\sum_{k \\neq i} \\exp(-||x_i - x_k||^2/2\\sigma^2)}$\n",
    "\n",
    "* Optimize the perplexity for each element i to give the target perplexity (provided in `utils.py`)\n",
    "\n",
    "* Symmetrize the affinity matrix using $p_{ij} = \\frac{p_{j|i}+p_{i|j}}{2N}$\n",
    "\n",
    "* Consider an initial embedding $Y^{0}$\n",
    "\n",
    "\n",
    "* Repeat for multiple iterations:\n",
    "\n",
    "   * Compute the affinities in the embedded space $q_{ij} = \\frac{(1 + ||y_i - y_j||^2)^{-1}}{\\sum_{k \\neq i}(1 + ||y_i - y_k||^2)^{-1}}$\n",
    "   \n",
    "   * Compute the gradient $\\frac{\\partial C}{\\partial Y}$ using $\\frac{\\partial C}{\\partial Y_i} = 4 \\sum_{j} (p_{ij}-q_{ij})(y_i-y_j)(1 + ||y_i - y_j||^2)^{-1}$\n",
    "   \n",
    "   * Update the embedding using the update rule $Y^{t} = Y^{t-1} + \\eta \\frac{\\partial C}{\\partial Y} + \\alpha (t)( Y^{t-1}-Y^{t-2})$, where $Y^{t}$ is the value of $Y$ at time $t$, where $Y^{t}=(0,0,...,0)$ for $t < 0$ and where $\\alpha (t) = 0.5$ at the beginning of the training procedure and $0.8$ towards the end.\n",
    "   \n",
    "\n",
    "* Return the final embedding $Y^{T}$ where $T$ is the number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing t-SNE (30P)\n",
    "\n",
    "You are asked to implement several functions that are used by the t-SNE algorithm. Their specification is given below. In their current form, they simply call functions of the module `solutions`, which is not provided. Replace these calls by your own implementation of the functions. Remark that most of the time, we work with log-probabilities. It is more convenient and numerically stable when the probabilities need to be defined or normalized. (See for example the function `scipy.misc.logsumexp` for that purpose.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy,scipy.spatial\n",
    "import numpy as np\n",
    "\n",
    "def student(Y):\n",
    "    # Calculate the join log-probabilities log(q_ij) defined above\n",
    "    #\n",
    "    # input:  Y    - An Nx2 array containing the embedding\n",
    "    # return: logQ - An NxN array containing log(q_ij)\n",
    "    \n",
    "    N = Y.shape[0]\n",
    "    D2 = scipy.spatial.distance.cdist(Y,Y,'sqeuclidean')\n",
    "    D = (1 + D2) ** -1\n",
    "    ulogQ = np.log(D)\n",
    "    logQ = ulogQ - np.log(D.sum() - N)\n",
    "    return logQ\n",
    "\n",
    "def objective(logP,logQ):\n",
    "    # Calculate the objective of t-SNE to minimize. The objective is the\n",
    "    # KL divergence C = KL(P||Q)\n",
    "    # \n",
    "    # inputs: logP - An NxN array containing log(p_ij)\n",
    "    #         logQ - An NxN array containing log(q_ij)\n",
    "    # return: C    - The value of the objective\n",
    "\n",
    "    C = np.sum(np.exp(logP) * (logP - logQ))\n",
    "    return C\n",
    "\n",
    "def gradient(logP,Y):\n",
    "    # Computes the gradient as described above.\n",
    "    #\n",
    "    #inputs: logP  - An NxN array containing log(p_ij)\n",
    "    #        Y     - An Nx2 array containing the embedding\n",
    "    #return: gradY - the gradient of the objective with respect to Y\n",
    "   \n",
    "    N = Y.shape[0]\n",
    "    gradY = np.zeros((N,2))\n",
    "    logQ = student (Y)\n",
    "    deltP = np.exp(logP) - np.exp(logQ)\n",
    "    D2 = scipy.spatial.distance.cdist(Y,Y,'sqeuclidean')\n",
    "    for i in range(N):\n",
    "        deltY = Y[i,:] - Y\n",
    "        for j in range (N):\n",
    "            gradY[i,:] += 4 * deltP[i,j] * deltY[j,:] / (1 + D2[i,j])\n",
    "    return gradY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below implements t-SNE algorithm. It takes as input some unsupervised dataset `X` (a `Nxd` array), and compute a two-dimensional embedding starting from an initial embedding `Y0` (a `Nx2` array). Various training parameters can be specified as optional parameters. The t-SNE algorithm makes use of the functions that are defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "\n",
    "def TSNE(X,Y0,perplexity=25,learningrate=1.0,nbiterations=250):\n",
    "\n",
    "    N,d = X.shape\n",
    "    \n",
    "    print('get affinity matrix')\n",
    "    \n",
    "    # get the affinity matrix in the original space\n",
    "    logP = utils.getaffinity(X,perplexity)\n",
    "    \n",
    "    # create initial embedding and update direction\n",
    "    Y  = Y0*1\n",
    "    dY = Y*0\n",
    "    \n",
    "    print('run t-SNE')\n",
    "    \n",
    "    for t in range(nbiterations):\n",
    "\n",
    "        # compute the pairwise affinities in the embedding space\n",
    "        logQ = student(Y)\n",
    "        \n",
    "        # monitor objective\n",
    "        if t %50 == 0: print('%3d %.3f'%(t,objective(logP,logQ)))\n",
    "        \n",
    "        # update\n",
    "        dY = (0.5 if t < 100 else 0.8)*dY + learningrate*gradient(logP,Y)\n",
    "        Y = Y - dY\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the T-SNE algorithm on the handwritten digits dataset, and compare the found embedding with simple PCA analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# read input dataset\n",
    "X,color=utils.get_data(mode=1)\n",
    "\n",
    "# run PCA\n",
    "U,W,_ = np.linalg.svd(X,full_matrices=False)\n",
    "Y0 = U[:,:2]*W[:2]\n",
    "plt.scatter(*Y0.T,c=color); plt.title('PCA')\n",
    "plt.show()\n",
    "\n",
    "# run TSNE starting with PCA embedding as an initial solution\n",
    "Y = TSNE(X,Y0,perplexity=10,learningrate=5.0)\n",
    "plt.scatter(*Y.T,c=color); plt.title('t-SNE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting With t-SNE (20P)\n",
    "\n",
    "The file `utils.py` contains a method `get_data(type)` that provides three datasets:\n",
    "\n",
    "* A collection of digits (less complex than MNIST)\n",
    "* Boston housing dataset\n",
    "* Iris dataset\n",
    "\n",
    "Using your implementation of t-SNE, and running it on the various dataset and with specific training parameters, answer the questions below. Along with your textual answers, include relevant results from running t-SNE in the code cell beneath each question where you should run code with certain parameters (perplexity, learning rate, choice of dataset) relevant to your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does perplexity and learning rate impact performance? What kind of extreme behaviour these parameters can cause?\n",
    "\n",
    "**[TODO: write your answer here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: write your code here and run it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of insight into the dataset you're dealing with can tSNE provide? Show one such example.\n",
    "\n",
    "**[TODO: write your answer here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: write your code here and run it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the embedding evolve during the optimization procedure (i.e. how are the clusters being formed progressively)?\n",
    "\n",
    "**[TODO: write your answer here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: write your code here and run it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
