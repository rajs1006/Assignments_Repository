{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "name": "",
  "signature": "sha256:772679553ba3f4c43f699f8d91b8dc23d5e5d8dac1bbaefef6259a84fa57e84b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
      "\n",
      "In this exercise, you will implement elements of the t-SNE algorithm described in the paper by Laurens van der Maaten (available on ISIS), and analyze its behavior. As a reminder, here are the main steps of the t-SNE procedure described in the paper:\n",
      "\n",
      "* compute pairwise affinities $p_{j|i}$ with perplexity perp using $p_{j|i} = \\frac{\\exp(-||x_i - x_j||^2/2\\sigma^2)}{\\sum_{k \\neq i} \\exp(-||x_i - x_k||^2/2\\sigma^2)}$\n",
      "\n",
      "* Optimize the perplexity for each element i to give the target perplexity (provided in `utils.py`)\n",
      "\n",
      "* Symmetrize the affinity matrix using $p_{ij} = \\frac{p_{j|i}+p_{i|j}}{2N}$\n",
      "\n",
      "* Consider an initial embedding $Y^{0}$\n",
      "\n",
      "\n",
      "* Repeat for multiple iterations:\n",
      "\n",
      "   * Compute the affinities in the embedded space $q_{ij} = \\frac{(1 + ||y_i - y_j||^2)^{-1}}{\\sum_{k \\neq i}(1 + ||y_i - y_k||^2)^{-1}}$\n",
      "   \n",
      "   * Compute the gradient $\\frac{\\partial C}{\\partial Y}$ using $\\frac{\\partial C}{\\partial Y_i} = 4 \\sum_{j} (p_{ij}-q_{ij})(y_i-y_j)(1 + ||y_i - y_j||^2)^{-1}$\n",
      "   \n",
      "   * Update the embedding using the update rule $Y^{t} = Y^{t-1} + \\eta \\frac{\\partial C}{\\partial Y} + \\alpha (t)( Y^{t-1}-Y^{t-2})$, where $Y^{t}$ is the value of $Y$ at time $t$, where $Y^{t}=(0,0,...,0)$ for $t < 0$ and where $\\alpha (t) = 0.5$ at the beginning of the training procedure and $0.8$ towards the end.\n",
      "   \n",
      "\n",
      "* Return the final embedding $Y^{T}$ where $T$ is the number of iterations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Implementing t-SNE (30P)\n",
      "\n",
      "You are asked to implement several functions that are used by the t-SNE algorithm. Their specification is given below. In their current form, they simply call functions of the module `solutions`, which is not provided. Replace these calls by your own implementation of the functions. Remark that most of the time, we work with log-probabilities. It is more convenient and numerically stable when the probabilities need to be defined or normalized. (See for example the function `scipy.misc.logsumexp` for that purpose.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import solutions\n",
      "\n",
      "def student(Y):\n",
      "    # Calculate the join log-probabilities log(q_ij) defined above\n",
      "    #\n",
      "    # input:  Y    - An Nx2 array containing the embedding\n",
      "    # return: logQ - An NxN array containing log(q_ij)\n",
      "    \n",
      "    logQ = solutions.student(Y)\n",
      "    return logQ\n",
      "\n",
      "def objective(logP,logQ):\n",
      "    # Calculate the objective of t-SNE to minimize. The objective is the\n",
      "    # KL divergence C = KL(P||Q)\n",
      "    # \n",
      "    # inputs: logP - An NxN array containing log(p_ij)\n",
      "    #         logQ - An NxN array containing log(q_ij)\n",
      "    # return: C    - The value of the objective\n",
      "\n",
      "    C = solutions.objective(logP,logQ)\n",
      "    return C\n",
      "\n",
      "def gradient(logP,Y):\n",
      "    # Computes the gradient as described above.\n",
      "    #\n",
      "    #inputs: logP  - An NxN array containing log(p_ij)\n",
      "    #        Y     - An Nx2 array containing the embedding\n",
      "    #return: gradY - the gradient of the objective with respect to Y\n",
      "\n",
      "    gradY = solutions.gradient(logP,Y)\n",
      "    return gradY"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The code below implements t-SNE algorithm. It takes as input some unsupervised dataset `X` (a `Nxd` array), and compute a two-dimensional embedding starting from an initial embedding `Y0` (a `Nx2` array). Various training parameters can be specified as optional parameters. The t-SNE algorithm makes use of the functions that are defined above."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import utils\n",
      "import numpy as np\n",
      "\n",
      "def TSNE(X,Y0,perplexity=25,learningrate=1.0,nbiterations=250):\n",
      "\n",
      "    N,d = X.shape\n",
      "    \n",
      "    print('get affinity matrix')\n",
      "    \n",
      "    # get the affinity matrix in the original space\n",
      "    logP = utils.getaffinity(X,perplexity)\n",
      "    \n",
      "    # create initial embedding and update direction\n",
      "    Y  = Y0*1\n",
      "    dY = Y*0\n",
      "    \n",
      "    print('run t-SNE')\n",
      "    \n",
      "    for t in range(nbiterations):\n",
      "\n",
      "        # compute the pairwise affinities in the embedding space\n",
      "        logQ = student(Y)\n",
      "        \n",
      "        # monitor objective\n",
      "        if t %50 == 0: print('%3d %.3f'%(t,objective(logP,logQ)))\n",
      "        \n",
      "        # update\n",
      "        dY = (0.5 if t < 100 else 0.8)*dY + learningrate*gradient(logP,Y)\n",
      "        Y = Y - dY\n",
      "\n",
      "    return Y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We test the T-SNE algorithm on the handwritten digits dataset, and compare the found embedding with simple PCA analysis."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import utils\n",
      "import numpy as np\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "# read input dataset\n",
      "X,color=utils.get_data(mode=1)\n",
      "\n",
      "# run PCA\n",
      "U,W,_ = np.linalg.svd(X,full_matrices=False)\n",
      "Y0 = U[:,:2]*W[:2]\n",
      "plt.scatter(*Y0.T,c=color); plt.title('PCA')\n",
      "plt.show()\n",
      "\n",
      "# run TSNE starting with PCA embedding as an initial solution\n",
      "Y = TSNE(X,Y0,perplexity=10,learningrate=5.0)\n",
      "plt.scatter(*Y.T,c=color); plt.title('t-SNE')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Experimenting With t-SNE (20P)\n",
      "\n",
      "The file `utils.py` contains a method `get_data(type)` that provides three datasets:\n",
      "\n",
      "* A collection of digits (less complex than MNIST)\n",
      "* Boston housing dataset\n",
      "* Iris dataset\n",
      "\n",
      "Using your implementation of t-SNE, and running it on the various dataset and with specific training parameters, answer the questions below. Along with your textual answers, include relevant results from running t-SNE in the code cell beneath each question where you should run code with certain parameters (perplexity, learning rate, choice of dataset) relevant to your answer."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How does perplexity and learning rate impact performance? What kind of extreme behaviour these parameters can cause?\n",
      "\n",
      "**[TODO: write your answer here]**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### TODO: write your code here and run it"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What kind of insight into the dataset you're dealing with can tSNE provide? Show one such example.\n",
      "\n",
      "**[TODO: write your answer here]**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### TODO: write your code here and run it"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How does the embedding evolve during the optimization procedure (i.e. how are the clusters being formed progressively)?\n",
      "\n",
      "**[TODO: write your answer here]**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### TODO: write your code here and run it"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}