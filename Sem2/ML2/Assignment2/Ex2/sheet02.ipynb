{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "name": "",
  "signature": "sha256:e5b07f8b68fe081fca3f959f8dc223dc2fae0e9d088f5b0a048a5817bc89fb43"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Locally Linear Embedding (40 P)\n",
      "\n",
      "In this assignment we will look at locally linear embedding and experiment with it on artificially generated datasets. The effects of neighbourhood size and noise on result quality will be analyzed.\n",
      "\n",
      "Information about the algorithm, publications and demos can be found at http://www.cs.nyu.edu/~roweis/lle/\n",
      "\n",
      "A guide for plotting can be found here: http://nbviewer.ipython.org/github/jrjohansson/scientific-python-lectures/blob/master/Lecture-4-Matplotlib.ipynb\n",
      "\n",
      "We first start by importing some basic python libraries for numerical computation and plotting."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib\n",
      "from matplotlib import pyplot as plt\n",
      "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
      "import scipy.spatial, scipy.linalg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The file `utils.py` contains several methods to generate pseudo-random three-dimensional datasets. They all have a low-dimensional manifold structure. The following code plots each dataset with default generation parameters (`N=1000` examples, and Gaussian noise of scale `0.25`)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import utils\n",
      "%matplotlib inline\n",
      "\n",
      "for dataset in [utils.spiral,utils.roll,utils.wave,utils.fold]:\n",
      "    print(dataset.__name__)\n",
      "    data,color = dataset(N=2000,noise=0.25)\n",
      "    plt.figure()\n",
      "    ax = plt.gca(projection='3d')\n",
      "    ax.view_init(elev=10., azim=120)\n",
      "    ax.scatter(data[:,0],data[:,1],data[:,2],c=color)\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Implementing LLE (20 P)\n",
      "\n",
      "**Implement a function `LLE(data,k)`**. The function takes as input the high-dimensinonal data the number of neighbors `k` used for reconstruction in the LLE algorithm. It returns the resulting 2D embedding (a two-dimensional array of size `Nx2`). A backbone of the implementation is given below. The implementation of LLE is described in the paper \"An Introduction to LLE\" linked from ISIS.\n",
      "\n",
      "*Reminder: During computation, you need to solve the problem Cw=**1**, where **1** is a column vector (1,1,...,1). In case k>d i.e. the size of the neighbourhood is larger than the number of dimensions of the space we're mapping to, it is necessary to regularize the matrix C. You can do this by adding positive terms on the diagonal. A good starting point is 0.05.*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Backbone implementation\n",
      "#def LLE(data,k):\n",
      "#    N = len(data)\n",
      "#    W = np.zeros([N,N]) # matrix for storing reconstruction weights\n",
      "#    M = np.zeros([N,N]) # matrix M of which eigenvectors are computed\n",
      "#    E = np.zeros([N,2]) # eigenvectors of M forming the embedding\n",
      "#    \n",
      "#    # Iterate over all data points to find their associated reconstruction weights\n",
      "#    for i in range(N):\n",
      "#        \n",
      "#        # 1. find nearest neighbors of data[i]\n",
      "#        \n",
      "#        # 2. compute local covariance (with diagonal regularization), and invert it\n",
      "#        \n",
      "#        # 3. compute reconstruction weights and store them in the row of the matrix W\n",
      "#\n",
      "#    # 4. Compute the matrix M from W and compute the desired eigenvectors E of M\n",
      "#    \n",
      "#    return E\n",
      "\n",
      "### TODO: Replace with your own implementation of LLE\n",
      "import solutions\n",
      "from solutions import LLE\n",
      "###"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Test your implementation by running the code below.** It plots the original data (projected on the first two dimensions), and next to it, the two-dimensional embedding. A correct implementation produces a two-dimensional dataset where the manifold is unfolded, and where nearby points in the embedding are also neighbors in the manifold, in particular, neighboring points in the plot should have similar color."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import utils\n",
      "%matplotlib inline\n",
      "data,color = utils.roll()\n",
      "embedding = LLE(data,k=20) \n",
      "fig = plt.figure(figsize=(10,5))\n",
      "\n",
      "ax = fig.add_subplot(1,2,1,projection='3d')\n",
      "ax.view_init(elev=10., azim=120)\n",
      "ax.scatter(data[:,0],data[:,1],data[:,2],c=color)\n",
      "ax.set_title('data')\n",
      "\n",
      "ax = fig.add_subplot(1,2,2)\n",
      "ax.scatter(embedding[:,0],embedding[:,1],c=color)\n",
      "ax.set_title('LLE')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Experiments (20 P)\n",
      "\n",
      "The function `compare(embed,dataset)`takes as input an embedding function and a dataset and plots the resulting embeddings for various choices of the parameter `k`, in particular, `k=5,20,80`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compare(embed, dataset):\n",
      "    cols = 4\n",
      "    \n",
      "    fig = plt.figure(figsize=(3*cols, 3))\n",
      "    \n",
      "    # Plot the data\n",
      "    data, color = dataset()\n",
      "    ax = fig.add_subplot(1, cols, 1,projection='3d')\n",
      "    ax.view_init(elev=10., azim=120)\n",
      "    ax.scatter(data[:,0],data[:,1],data[:,2],c=color)\n",
      "    \n",
      "    ax.set_title('data')\n",
      "    ax.set_xticks([], [])\n",
      "    ax.set_yticks([], [])\n",
      "    ax.set_zticks([], [])\n",
      "\n",
      "    # Plot embeddings with various parameters K\n",
      "    for i, k in enumerate([5,20,80]):\n",
      "        ax = fig.add_subplot(1, cols, 2+i)\n",
      "\n",
      "        z = embed(data,k=k)\n",
      "\n",
      "        ax.scatter(z[:,0], z[:,1], c=color)\n",
      "        ax.set_title('LLE, k=%d'%k)\n",
      "        ax.set_xticks([], [])\n",
      "        ax.set_yticks([], [])\n",
      "    plt.tight_layout()\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Datasets and optimal parameter `k`\n",
      "\n",
      "The code below tests the LLE embedding algorithm on each dataset: spiral, roll, wave, and fold. **Explain** what is a good parameter `k` of the LLE algorithm, and how this parameter relates to the various properties of the dataset.\n",
      "\n",
      "**[TODO: WRITE YOUR ANSWER HERE]**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for dataset in [utils.spiral,utils.roll,utils.wave,utils.fold]:\n",
      "    print(dataset.__name__)\n",
      "    compare(LLE,dataset)"
     ],
     "language": "python",
     "metadata": {
      "scrolled": false
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### LLE and the effect of noise\n",
      "\n",
      "The code below tests LLE on the spiral dataset with three different levels of noise. **Describe** how the noise affects the embedding.\n",
      "\n",
      "**[TODO: WRITE YOUR ANSWER HERE]**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for noise in [0.1,0.3,1.0]:\n",
      "    print('noise=%.3f'%noise)\n",
      "    dataset = lambda: utils.spiral(noise=noise)\n",
      "    compare(LLE,dataset)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Discussion\n",
      "\n",
      "* When applying LLE to a 200-dimensional dataset which can't be visualized, how would you assess whether the found embedding is good? Discuss how you would do it or argue why it can't be done.\n",
      "\n",
      " **[TODO: WRITE YOUR ANSWER HERE]**\n",
      "\n",
      "\n",
      "* Could utilizing this technique in conjuction with a classifier improve its performance? Which classifiers, if any, would benefit the most?\n",
      "\n",
      " **[TODO: WRITE YOUR ANSWER HERE]**"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}