{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cognitive Algorithms - Assignment 3 (30 points)\n",
    "Cognitive Algorithms        \n",
    "Summer term 2018      \n",
    "Technische Universit√§t Berlin     \n",
    "Fachgebiet Maschinelles Lernen \n",
    "\n",
    "**Due on May 23, 2018 10am via ISIS **\n",
    "                    \n",
    "After completing all tasks, run the whole notebook so that the content of each cell is properly displayed. Make sure that the code was ran and the entire output (e.g. figures) is printed. Print the notebook as a PDF file and again make sure that all lines are readable - use line breaks in the Python Code '\\' if necessary. Points will be deducted, if code or content is not readable!                  \n",
    "           \n",
    "**Upload the PDF file that contains a copy of your notebook on ISIS.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group:        \n",
    "Members:          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Theory (13 points)\n",
    "---\n",
    "### Task 1: Multiple Choice Questions (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A)** The goal of LDA is to find a $\\mathbf w \\in \\mathbb{R}^d$ that ...\n",
    "- [ ] minimizes mean class difference and minimizes variance in each class              \n",
    "- [ ] minimizes mean class difference and maximizes variance in each class                    \n",
    "- [ ] maximizes mean class difference and maximizes variance in each class                \n",
    "- [ ] maximizes mean class difference and minimizes variance in each class                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B)** Below you can see a figure that shows a data set of two classes (blue and yellow) and three different lines. Assume NCC is trained on the given data. Which line corresponds to the resulting decision boundary of NCC.\n",
    "- [ ] The black line resembles the decision boundary given by NCC.              \n",
    "- [ ] The red line resembles the decision boundary given by NCC.               \n",
    "- [ ] The green line resembles the decision boundary given by NCC.              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure_1](Figure_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Covariance (11 points)\n",
    "Let $X$ and $Y$ be two random variables. In the lecture you learned about covariance and correlation.\n",
    "$$\\text{Cov}(X,Y) = \\mathbb{E}(((X-\\mathbb{E}(X))(Y-\\mathbb{E}(Y)))$$\n",
    "$$\\text{Corr}(X,Y) = \\frac{\\text{Cov}(X,Y)}{ \\sqrt{\\mathbb{V}(X)} \\sqrt{\\mathbb{V}(Y)} }$$\n",
    "\n",
    "**A) (1 point)** Let $X$ be a random variable. Show that \n",
    "$$\\text{Cov}(X,X) = \\mathbb{V}(X)$$\n",
    "where the variance of a random variable is defined as \n",
    "$$\\mathbb{V}(X) =  \\mathbb{E}((X-\\mathbb{E}(X))^2) = \\mathbb{E}(X^2) - \\mathbb{E}(X)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your proof for A) here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B) (1 point)** Use your results from A) to calculate the correlation\n",
    "$$\\text{Corr}(X,X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your derivation for B) here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C) (3 points)** Show that the algebraic formula for the variance can be generalized to covariance, i.e. show for two random variables $X$ and $Y$ that the covariance can be simplified to\n",
    "$$\\text{Cov}(X,Y) = \\mathbb{E}(XY) - \\mathbb{E}(X) \\mathbb{E}(Y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your proof for C) here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D) (6 points)** Let $X \\in \\mathbb R^{D \\times N}$ be a data matrix that holds for each random variable $X_1, \\ldots X_D \\;$ $N$ observations, i.e. $X_d \\in \\mathbb{R}^N$. Use your results from task A)-C) to show, that if the data is centered ($\\forall_{d=1}^{D} \\mathbb{E}(X_d) = 0$) the empirical estimate of the covariance matrix is given by $S$, i.e.\n",
    "$$\\Sigma = \\left( \\begin{array}{rrrr} \\text{Cov}(X_1,X_1) & \\text{Cov}(X_1,X_2) & \\ldots & \\text{Cov}(X_1,X_D) \\\\ \\text{Cov}(X_2,X_1) & \\text{Cov}(X_2,X_2) & \\ldots & \\text{Cov}(X_2,X_D) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\text{Cov}(X_D,X_1) & \\ldots & \\ldots & \\text{Cov}(X_D,X_D) \\end{array}\t\\right) \\approx \\frac{1}{N} X X^T = S$$\n",
    "*Hint:* Use the following properties     \n",
    "- $\\mathbb{E}(X_d) \\approx \\frac{1}{N} \\sum_{n=1}^{N} X_{d,n} = 0$               \n",
    "- $\\mathbb{E}(X_d X_{d'}) \\approx \\frac{1}{N} \\sum_{n=1}^{N} X_{d,n} X_{d',n}$                  \n",
    "- $\\text{Cov}(X_d, X_{d'}) = \\text{Cov}(X_{d'}, X_d)$                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your proof for D) here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Programming (17 points)\n",
    "---\n",
    "\n",
    "In this assignment you will compare the three linear classification algorithms that you encountered in the lecture - the Perceptron, the Nearest Centroid Classifier (NCC) and the Linear Discriminant Analysis (LDA). This comparision is done on a toy data set and on two different real data sets - the USPS data from the last assignment, and a Brain-Computer Interface (BCI) data set. \n",
    "\n",
    "Download the ```usps.mat``` and ```bcidata.mat``` data sets from the ISIS web site, if not done yet. Your task will be to implement LDA and use the provided code to analyse the data. \n",
    "\n",
    "The BCI data set consists of preprocessed EEG data $X \\in\\mathbb{R}^{5 \\times 62 \\times 5322}$ and stimulus labels $Y \\in\\mathbb R^{2 \\times 5322}$ during a copy-spelling paradigm with a P300 speller. The data matrix $X$ contains 5 selected time windows of EEG activity at 62 electrodes after a visual stimulus was presented on the screen in front of the participant. If the first row of $Y$ is 1, the stimulus was a target stimulus, if the second row of $Y$ is 1, the stimulus was a non-target stimulus. The goal is to predict if the simulus was a target or not given the EEG.        \n",
    "*Below you can find the provided code. Change the code only where indicated. See Part 2 Task A) for more information.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import scipy.io as io\n",
    "from scipy.linalg import inv\n",
    "import pylab as pl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_lda(X,Y):\n",
    "    ''' Trains a linear discriminant analysis\n",
    "    Definition:  w, b   = train_lda(X,Y)\n",
    "    Input:       X       -  DxN array of N data points with D features\n",
    "                 Y       -  1D array of length N of class labels {-1, 1}\n",
    "    Output:      w       -  1D array of length D, weight vector  \n",
    "                 b       -  bias term for linear classification                          \n",
    "    '''\n",
    "    # your code here \n",
    "    # hint: use the scipy/numpy function sp.cov\n",
    "    \n",
    "    \n",
    "def load_usps_data(fname, digit=3):\n",
    "    ''' Loads USPS (United State Postal Service) data from <fname> \n",
    "    Definition:  X, Y = load_usps_data(fname, digit = 3)\n",
    "    Input:       fname   - string\n",
    "                 digit   - optional, integer between 0 and 9, default is 3\n",
    "    Output:      X       -  DxN array with N images with D pixels\n",
    "                 Y       -  1D array of length N of class labels\n",
    "                             (1 - picture displays <digit>, -1 - otherwise)                           \n",
    "    '''\n",
    "    # load the data\n",
    "    data = io.loadmat(fname)\n",
    "    # extract images and labels\n",
    "    X = data['data_patterns']\n",
    "    Y = data['data_labels']\n",
    "    Y = Y[digit,:]\n",
    "    return X, Y\n",
    "    \n",
    "    \n",
    "def load_bci_data(fname):\n",
    "    ''' Loads BCI data (one subject, copy-spelling experiment) from <fname> \n",
    "    Definition:  X, Y = load_bci_data(fname)\n",
    "    Input:       fname   - string\n",
    "    Output:      X       -  DxN array with N images with D pixels\n",
    "                 Y       -  1D array of length N of class labels \n",
    "                            (1- target, -1 - non-target)                         \n",
    "    '''\n",
    "    # load the data\n",
    "    data = io.loadmat(fname)\n",
    "    # extract time-electrode features and labels\n",
    "    X = data['X']\n",
    "    Y = data['Y']\n",
    "    # collapse the time-electrode dimensions\n",
    "    X = sp.reshape(X,(X.shape[0]*X.shape[1],X.shape[2]))\n",
    "    # transform the labels to (-1,1)\n",
    "    Y = sp.sign((Y[0,:]>0) -.5)\n",
    "    return X,Y\n",
    "\n",
    "def train_perceptron(X,Y,iterations=200,eta=.1):\n",
    "    ''' Trains a linear perceptron\n",
    "    Definition:  w, b, acc  = train_perceptron(X,Y,iterations=200,eta=.1)\n",
    "    Input:       X       -  DxN array of N data points with D features\n",
    "                 Y       -  1D array of length N of class labels {-1, 1}\n",
    "                 iter    -  optional, number of iterations, default 200\n",
    "                 eta     -  optional, learning rate, default 0.1\n",
    "    Output:      w       -  1D array of length D, weight vector \n",
    "                 b       -  bias term for linear classification                          \n",
    "    '''\n",
    "    #include the bias term by adding a row of ones to X \n",
    "    X = sp.concatenate((sp.ones((1,X.shape[1])), X))\n",
    "    #initialize weight vector\n",
    "    weights = sp.ones((X.shape[0]))/X.shape[0]\n",
    "    for it in sp.arange(iterations):\n",
    "        # indices of misclassified data\n",
    "        wrong = (sp.sign(weights.dot(X)) != Y).nonzero()[0]\n",
    "        if wrong.shape[0] > 0:\n",
    "            # pick a random misclassified data point\n",
    "            m = wrong[sp.random.randint(0, wrong.shape[0]-1)]\n",
    "            #update weight vector (use variable learning rate (eta/(1.+it)) )\n",
    "            weights = weights  + (eta/(1.+it)) * X[:, m] * Y[m]; \n",
    "            # compute accuracy\n",
    "            wrong = (sp.sign(weights.dot(X)) != Y).nonzero()[0]\n",
    "    b = -weights[0] \n",
    "    w = weights[1:]\n",
    "    return w,b\n",
    "\n",
    "def train_ncc(X,Y):\n",
    "    ''' Trains a nearest centroid classifier\n",
    "    Definition:  w, b   = train_ncc(X,Y)\n",
    "    Input:       X       -  DxN array of N data points with D features\n",
    "                 Y       -  1D array of length N of class labels {-1, 1}\n",
    "    Output:      w       -  1D array of length D, weight vector  \n",
    "                 b       -  bias term for linear classification                          \n",
    "    '''\n",
    "    #class means\n",
    "    mupos = sp.mean(X[:,Y>0],axis=1)\n",
    "    muneg = sp.mean(X[:,Y<0],axis=1)\n",
    "    #weight vector and bias term\n",
    "    w = mupos - muneg\n",
    "    b = (w.dot(mupos) + w.dot(muneg))/2.\n",
    "    return w,b\n",
    "    \n",
    "def plot_histogram(X, Y, w, b, cname):\n",
    "    ''' Plots a histogram of classifier outputs (w^T X) for each class \n",
    "    Input:          X       -  DxN array of N data points with D features\n",
    "                    Y       -  1D array of length N of class labels\n",
    "                    w       -  1D array of length D, weight vector \n",
    "                    b       -  bias term for linear classification  \n",
    "                    cname   - name of the classifier \n",
    "    '''\n",
    "    pl.hist((w.dot(X[:,Y<0]), w.dot(X[:,Y>0])))\n",
    "    pl.xlabel(\"w^T X\")\n",
    "    pl.title(cname + ' ' + str(100*sp.sum(sp.sign(w.dot(X)-b)==Y)/X.shape[1]) + \"%\")   \n",
    "     \n",
    "def compare_classifiers_toy():\n",
    "    '''\n",
    "    Compares 3 different linear classifiers (Nearest-Centroid, Linear Discriminant Analysis, \n",
    "    Perceptron) on 2 dimensional toy data\n",
    "    '''\n",
    "    #generate 2D data\n",
    "    N =500\n",
    "    cov = sp.array([[5, 0], [0, 0.5]])\n",
    "    x1 = sp.random.multivariate_normal([-0.5, -0.5], cov, N) \n",
    "    x2 = sp.random.multivariate_normal([2.5, 0.5], cov, N) \n",
    "    X = sp.vstack((x1, x2)).transpose()\n",
    "    Y = sp.hstack((sp.ones((N)), -1*sp.ones((N))))\n",
    "    \n",
    "    #train NCC, LDA and Perceptron\n",
    "    w_ncc,b_ncc = train_ncc(X,Y)\n",
    "    w_lda,b_lda = train_lda(X,Y)\n",
    "    w_per,b_per = train_perceptron(X,Y)\n",
    "       \n",
    "    #plot result\n",
    "    pl.figure()\n",
    "    b_ncc = 10*b_ncc / sp.linalg.norm(w_ncc)\n",
    "    b_lda = 10*b_lda / sp.linalg.norm(w_lda)\n",
    "    b_per = 10*b_per / sp.linalg.norm(w_per)\n",
    "    w_lda = 10*w_lda / sp.linalg.norm(w_lda)\n",
    "    w_ncc = 10*w_ncc / sp.linalg.norm(w_ncc)\n",
    "    w_per = 10*w_per / sp.linalg.norm(w_per)\n",
    "    pl.plot([-w_lda[1], w_lda[1]], [w_lda[0]+b_lda/w_lda[1], -w_lda[0]+b_lda/w_lda[1]], \n",
    "        color = 'k', label='LDA: Acc ' + str(100*sp.sum(sp.sign(w_lda.dot(X)-b_lda)==Y)/X.shape[1]) + \"%\")\n",
    "    pl.plot([-w_ncc[1], w_ncc[1]], [w_ncc[0]+b_ncc/w_ncc[1], -w_ncc[0]+b_ncc/w_ncc[1]], \n",
    "        color = 'r', linestyle = '--', label='NCC: Acc ' + str(100*sp.sum(sp.sign(w_ncc.dot(X)-b_ncc)==Y)/X.shape[1]) + \"%\")\n",
    "    pl.plot([-w_per[1], w_per[1]], [w_per[0]+b_per/w_per[1], -w_per[0]+b_per/w_per[1]], \n",
    "        color = 'g', linestyle = ':', label='PER: Acc ' + str(100*sp.sum(sp.sign(w_per.dot(X)-b_per)==Y)/X.shape[1]) + \"%\")\n",
    "    pl.plot(x1[:,0], x1[:,1], 'y+')\n",
    "    pl.plot(x2[:,0], x2[:,1], 'b+')\n",
    "    pl.axis('equal')\n",
    "    pl.legend(loc=1)\n",
    "\n",
    "def compare_classifiers(usps = True, digit = 3):\n",
    "    '''\n",
    "    Compares 3 different linear classifiers (Nearest-Centroid, Linear Discriminant Analysis, \n",
    "    Perceptron) on either USPS data (for usps=True) or on BCI data (for usps = False)\n",
    "    '''\n",
    "    if usps: #load usps data set\n",
    "        X,Y = load_usps_data('usps.mat',digit)\n",
    "        tit = 'USPS(' + str(digit) + ')'\n",
    "    else: #load bci data set \n",
    "        X,Y = load_bci_data('bcidata.mat')\n",
    "        tit = 'BCI'\n",
    "    \n",
    "    #Use crossvalidation to estimate the training and test accuracies\n",
    "    acc_cv = sp.zeros((5, 6))\n",
    "    (acc_cv[:,0],acc_cv[:,1]) = crossvalidate(X,Y,trainfun=train_ncc)\n",
    "    (acc_cv[:,2],acc_cv[:,3]) = crossvalidate(X,Y,trainfun=train_lda)\n",
    "    (acc_cv[:,4],acc_cv[:,5]) = crossvalidate(X,Y,trainfun=train_perceptron)\n",
    "        \n",
    "    #Plot the crossvalidation output\n",
    "    pl.figure()\n",
    "    ax1 = pl.subplot2grid((2,3), (0,0), colspan = 3)\n",
    "    pl.bar(sp.array([1, 2, 3, 4, 5, 6]) - 0.4,  acc_cv.mean(0), width = 0.8,\n",
    "        yerr =  acc_cv.std(0), ecolor = 'k', color = 'g')\n",
    "    pl.xticks([1, 2, 3, 4, 5, 6], ['NCC tain', 'NCC test', 'LDA train', 'LDA test', \n",
    "        'PER train', 'PER test'])\n",
    "    pl.xlim([0, 7])\n",
    "    pl.ylim([0.5, 1])\n",
    "    pl.ylabel('CV Accuracy')\n",
    "    pl.title(tit + ' data set')\n",
    "\n",
    "    #Train the classifiers and plot the output histograms\n",
    "    w_ncc,b_ncc = train_ncc(X,Y)\n",
    "    w_lda,b_lda = train_lda(X,Y)\n",
    "    w_per,b_per= train_perceptron(X,Y)\n",
    "    \n",
    "    ax2 = pl.subplot2grid((2,3), (1,0))\n",
    "    plot_histogram(X, Y, w_ncc, b_ncc, 'NCC')\n",
    "    ax3 = pl.subplot2grid((2,3), (1,1))\n",
    "    plot_histogram(X, Y, w_lda, b_lda, 'LDA')\n",
    "    ax4 = pl.subplot2grid((2,3), (1,2))\n",
    "    plot_histogram(X, Y, w_per, b_per, 'PER')\n",
    "\n",
    "def crossvalidate(X,Y, f=5, trainfun=train_ncc):\n",
    "    ''' \n",
    "    Test generalization performance of a linear classifier by crossvalidation\n",
    "    Definition:     crossvalidate(X,Y, f=5, trainfun=train_ncc)\n",
    "    Input:      X        -  DxN array of N data points with D features\n",
    "                Y        -  1D array of length N of class labels\n",
    "                f        - number of cross-validation folds\n",
    "                trainfun - function for linear classification training\n",
    "    Output:     acc_train - (f,) array of accuracies in test train folds\n",
    "                acc_test  - (f,) array of accuracies in each test fold\n",
    "    '''\n",
    "    N = f*(X.shape[-1]/f)\n",
    "    idx = sp.reshape(sp.arange(N),(f,N/f))\n",
    "    acc_train = sp.zeros((f))\n",
    "    acc_test = sp.zeros((f))\n",
    "    \n",
    "    for ifold in sp.arange(f):\n",
    "        testidx = sp.zeros((f),dtype=bool)\n",
    "        testidx[ifold] = 1\n",
    "        test = idx[testidx,:].flatten()\n",
    "        train = idx[~testidx,:].flatten()\n",
    "        w,b = trainfun(X[:,train],Y[train])\n",
    "        acc_train[ifold] = sp.sum(sp.sign(w.dot(X[:,train])-b)==Y[train])/sp.double(train.shape[0])\n",
    "        acc_test[ifold] = sp.sum(sp.sign(w.dot(X[:,test])-b)==Y[test])/sp.double(test.shape[0])\n",
    "    \n",
    "    return acc_train,acc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A) (7 points)** Implement a linear discriminant analysis (LDA) classifer by completing the function stub  ```train_lda```, that is, find a \n",
    "vector $\\mathbf{w}$ such that  \n",
    "$$\\mathbf{w} = \\underset{\\mathbf{w}}{\\text{argmax}} \\; \\frac{\\mathbf{w}^T S_B \\mathbf{w}}{\\mathbf{w}^T S_{W} \\mathbf{w}}$$\n",
    "where $S_B$ denotes the 'between-class scatter' and $S_W$ denotes the 'within-class scatter'\n",
    "\\begin{eqnarray*}\n",
    "S_B &= & (\\mathbf w_+ - \\mathbf w_{-})(\\mathbf w_+ - \\mathbf w_{-})^T \\\\\n",
    "S_W &= & \\frac{1}{N_{+}} \\sum_{i=1}^{N_{+}}(\\mathbf x_{+i} - \\mathbf w_{+})  (\\mathbf x_{+i} - \\mathbf w_{+})^T + \\frac{1}{N_{-}} \\sum_{i=1}^{N_{-}}(\\mathbf x_{-i} - \\mathbf w_{-})  (\\mathbf x_{-i} - \\mathbf w_{-})^T\n",
    "\\end{eqnarray*}\n",
    "and $\\mathbf w_{+}$, $\\mathbf w_{-}$ denote the respective class means. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B) (5 points)** Test your LDA implementation with the provided function ```compare_classifiers_toy```. It generates a 2D toy data set and plots the resulting separating hyperplanes for the three linear classification methods. Answer the following short questions: \n",
    "- Run the function several times - what do you notice for the Perceptron as compared to NCC or LDA? In one sentence, explain the behaviour of the perceptron. \n",
    "- Have a look in the code how the toy data is generated - is LDA optimal for this type of data? Why?\n",
    "- How would you have to change the data generation such that NCC and LDA yield the same result? Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your answers for B) here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compare_classifiers_toy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C) (1 points)** Call ```compare_classifiers``` for a digit of your choice of the USPS data set, as well as for the BCI data. It plots the histogram of classifier outputs and the classification accuracies for the NCC, the LDA and the perceptron. Which algorithm (Nearest Centroid Classifier, Linear Discriminant Analysis or Perceptron) would you prefer for which task? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your answers for C) here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compare_classifiers()\n",
    "compare_classifiers(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D) (4 points)** Briefly explain in your own words how crossvalidation is done. To do so, you can examine the function ```crossvalidate```. When we want to compare the performance of different classifiers, which values should we look at - the train or the test accuracies? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your answers for D) here]**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
