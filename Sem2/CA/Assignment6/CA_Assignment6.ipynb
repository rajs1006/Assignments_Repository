{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cognitive Algorithms - Assignment 6 (30 points)\n",
    "Cognitive Algorithms        \n",
    "Summer term 2018      \n",
    "Technische Universität Berlin     \n",
    "Fachgebiet Maschinelles Lernen \n",
    "\n",
    "**Due on July 16, 2018 10am via ISIS **\n",
    "                    \n",
    "After completing all tasks, run the whole notebook so that the content of each cell is properly displayed. Make sure that the code was ran and the entire output (e.g. figures) is printed. Print the notebook as a PDF file and again make sure that all lines are readable - use line breaks in the Python Code '\\' if necessary. Points will be deducted, if code or content is not readable!                  \n",
    "           \n",
    "**Upload the PDF file that contains a copy of your notebook on ISIS.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group:     \n",
    "Members:     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Theory (15 points)\n",
    "---\n",
    "### Task 1: Multiple Choice Questions (2 points)\n",
    "**A)** A Multilayer Perceptron can be used for ...           \n",
    "- [X] classification                   \n",
    "- [X] regression                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B)** The training algorithm of the MLP mainly consists of two phases. Which statement about the backward phase is true?       \n",
    "- [X] the error of each neuron is computed for each neuron, starting with the neurons in the output layer          \n",
    "- [ ] the error of each neuron is computed for each neuron, starting with the neurons in the input layer               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Learning Procedure (5 points)\n",
    "Before we can use an MLP for a given task, we have to train it. This training procedure (here: batch mode) is composed of different steps, that you can find below. However, the order of the steps is not correct. Please bring the steps in the correct order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. FOR EACH input vector\n",
    "1. END FOR EACH\n",
    "1. REPEAT until stopping criterion is fulfilled\n",
    "1. END REPEAT\n",
    "1. compute the error of the neurons in the hidden layer\n",
    "1. update the hidden layer weights\n",
    "1. Initialize all weights\n",
    "1. compute the activation of each neuron of the hidden layer\n",
    "1. update the output layer weights\n",
    "1. compute the error of the output neuron\n",
    "1. compute the activation of the output layer neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your solution for task 2 here]**       \n",
    "1.  Initialize all weight            \n",
    "2.  Repeat until stopping criterin is fulffiled.             \n",
    "3.  FOR EACH input vector\n",
    "4.  compute the activation of each neuron of the hidden layer\n",
    "5.  compute the activation of the output layer neurons\n",
    "6.  compute the error of the output neuron\n",
    "7.  compute the error of the neurons in the hidden layer\n",
    "8.  update the hidden layer weights\n",
    "9.  update the output layer weights\n",
    "10. END FOR EACH\n",
    "11. END REPEAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Linear Activation Function (8 points)\n",
    "It can be shown, if a multilayer perceptron has a linear activation function in all neurons, any number of layers can be reduced to a two-layer input-output model.                                         \n",
    "Consider an MLP with $N$ input neurons $x_n$, one hidden layer with $K$ neurons $z_k$ and a single output $y$. ${\\alpha_k}_n$ define the weights connecting neuron $x_n$ and $z_k$, and $\\beta_k$ the weights connecting $z_k$ and the output. ${\\alpha_k}_0$ and $\\beta_0$ are the weights of the biases. All neurons have a linear activation function, thus the output of a hidden neuron can be written as \n",
    "$$z_k = -{\\alpha_k}_0 + \\sum_{n=1}^{N} {\\alpha_k}_n x_n$$\n",
    "and the total output becomes \n",
    "$$y = -\\beta_0 + \\sum_{k=1}^{K} \\beta_k z_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A) (4 points)** Draw a graph representing the MLP and annotate it with the relevant variables (input, hidden and output neurons, bias and weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your answer for 3A here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B) (4 points)** Show that there exists an MLP without hidden layers, which models the same function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your answer for 3B here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Programming (15 points)\n",
    "---\n",
    "Like in the first assignment you aim to recognize handwritten digits. This time you will not train a linear perceptron, but a non-linear multilayer perceptron (MLP). You won’t have to implement it – we just want you to play around with existing code and modify it slightly. We are using the ```scikit-learn``` implementation, that can be found here:            \n",
    "http://scikit-learn.org/stable/modules/neural_networks_supervised.html            \n",
    "You might have to install ```scikit-learn``` beforehand. Follow the instructions on their webpage to do so.                   \n",
    "This time we will use the full MNIST Data set.             \n",
    "\n",
    "Below you find the code to load the MNIST dataset and to train an MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import convolve\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "import os.path\n",
    "import scipy as sp\n",
    "import pylab as pl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and loading MNIST data\n",
      "Got MNIST with 52500 training- and 17500 test samples\n",
      "Digit distribution in whole dataset: [6903 7877 6990 7141 6824 6313 6876 7293 6825 6958]\n",
      "Training model.\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed: 10.3min\n"
     ]
    }
   ],
   "source": [
    "PATH = 'mlp_model.pkl'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('Fetching and loading MNIST data')\n",
    "    mnist = fetch_mldata('MNIST original')\n",
    "    X, y = mnist.data, mnist.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X / 255., y, test_size=0.25, random_state=0)\n",
    "\n",
    "    print('Got MNIST with %d training- and %d test samples' % (len(y_train), len(y_test)))\n",
    "    print('Digit distribution in whole dataset:', np.bincount(y.astype('int64')))\n",
    "\n",
    "    clf = None\n",
    "    if os.path.exists(PATH):\n",
    "        print('Loading model from file.')\n",
    "        clf = joblib.load(PATH).best_estimator_\n",
    "    else:\n",
    "        print('Training model.')\n",
    "        params = {'hidden_layer_sizes': [(256,), (512,), (128, 256, 128,)]}\n",
    "        mlp = MLPClassifier(verbose=10, learning_rate='adaptive')\n",
    "        clf = GridSearchCV(mlp, params, verbose=10, n_jobs=-1, cv=5)\n",
    "        clf.fit(X_train, y_train)\n",
    "        print('Finished with grid search with best mean cross-validated score:', clf.best_score_)\n",
    "        print('Best params appeared to be', clf.best_params_)\n",
    "        joblib.dump(clf, PATH)\n",
    "        clf = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A) (4 points)** Shortly explain in your own words, what the code does. Ideally explain it line-by-line (```print``` statements can be omitted). You can write short comments directly in the code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B) (1 point)**  Run the code (this may take a while when running it for the first time). What are the training and testing errors?         \n",
    "*Hint: The current progress is printed on the Jupyter Notebook terminal.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(clf.score(X_train, y_train))\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training error is **~[training error]**.                      \n",
    "The test error is **~[test error]**.             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C) (4 points)** What does ```GridSearchCV``` do? Do we really need this function? Explain your decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your answer for C here]**         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D) (3 points)** What role plays the ```random_state``` parameter in ```train_test_split```? What happens if we left it out?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your answer for D here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E) (3 points)** We now want to compare an MLP without any hidden units with a single Perceptron. To do so, first train an MLP without an hidden layer by changing the given code. Print its training and test error. Compare the result to the one you obtained when training the Perceptron on the ```USPS``` dataset (assignment 2). Is this MLP then the same algorithm as the Perceptron of assignment 2? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code for task E) here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training error is **~[training error]**.                   \n",
    "The test error is **~[test error]**.              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your answer for E here]**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
